{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    lower = 1e-12\n",
    "    return F.softplus(x) + lower\n",
    "\n",
    "def lengthscales(var):\n",
    "    return softplus(var)\n",
    "\n",
    "def variance(var):\n",
    "    return softplus(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_dist_dimwise(X, X2=None):\n",
    "    \"\"\"\n",
    "    Computes squared euclidean distance (scaled) for dimwise kernel setting\n",
    "    @param X: Input 1 (N,D_in)\n",
    "    @param X2: Input 2 (M,D_in)\n",
    "    @return: Tensor (D_out, N,M)\n",
    "    \"\"\"\n",
    "    X = X.unsqueeze(0) / lengthscales(unconstrained_lengthscales).unsqueeze(1)  # (D_out,N,D_in)\n",
    "    Xs = torch.sum(torch.pow(X, 2), dim=2)  # (D_out,N)\n",
    "    if X2 is None:\n",
    "        return -2 * torch.einsum('dnk, dmk -> dnm', X, X) + \\\n",
    "                Xs.unsqueeze(-1) + Xs.unsqueeze(1)  # (D_out,N,N)\n",
    "    else:\n",
    "        X2 = X2.unsqueeze(0) / lengthscales(unconstrained_lengthscales).unsqueeze(1)  # (D_out,M,D_in)\n",
    "        X2s = torch.sum(torch.pow(X2, 2), dim=2)  # (D_out,N)\n",
    "        return -2 * torch.einsum('dnk, dmk -> dnm', X, X2) + Xs.unsqueeze(-1) + X2s.unsqueeze(1)  # (D_out,N,M)\n",
    "\n",
    "def square_dist(X, X2=None):\n",
    "    \"\"\"\n",
    "    Computes squared euclidean distance (scaled) for non dimwise kernel setting\n",
    "    @param X: Input 1 (N,D_in)\n",
    "    @param X2: Input 2 (M,D_in)\n",
    "    @return: Tensor (N,M)\n",
    "    \"\"\"\n",
    "    X = X / lengthscales(unconstrained_lengthscales)  # (N,D_in)\n",
    "    Xs = torch.sum(torch.pow(X, 2), dim=1)  # (N,)\n",
    "    if X2 is None:\n",
    "        return -2 * torch.matmul(X, X.t()) + \\\n",
    "                torch.reshape(Xs, (-1, 1)) + torch.reshape(Xs, (1, -1))  # (N,1)\n",
    "    else:\n",
    "        X2 = X2 / lengthscales(unconstrained_lengthscales)  # (M,D_in)\n",
    "        X2s = torch.sum(torch.pow(X2, 2), dim=1)  # (M,)\n",
    "        return -2 * torch.matmul(X, X2.t()) + torch.reshape(Xs, (-1, 1)) + torch.reshape(X2s, (1, -1))  # (N,M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_matrix_a(X, X2=None):\n",
    "    '''\n",
    "    Computes (X-X2)\n",
    "    '''\n",
    "    X = X / lengthscales(unconstrained_lengthscales)  # (N,D_in)\n",
    "    if X2 is None:\n",
    "        X2=X\n",
    "    else:\n",
    "        X2 = X2 / lengthscales(unconstrained_lengthscales) # (M,D_in)\n",
    "    return X[:,None,:] - X2[None,:,:] #broadcasting rules (M,N, D_in)\n",
    "\n",
    "def difference_matrix_dimwise(X, X2=None):\n",
    "    '''\n",
    "    Computes (X-X2)\n",
    "    '''\n",
    "    X = X.unsqueeze(0) / lengthscales(unconstrained_lengthscales).unsqueeze(1)   # (D_out,N,D_in)\n",
    "    if X2 is None:\n",
    "        X2=X\n",
    "    else:\n",
    "        X2 = X2.unsqueeze(0) / lengthscales(unconstrained_lengthscales).unsqueeze(1)  # (D_out,M,D_in)\n",
    "    return X[:,:,None,:] - X2[:,None,:,:] #broadcasting rules (D_out, M, N, D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(X, X2=None):\n",
    "    if X2 is None:\n",
    "        return torch.eye(X.shape[0])\n",
    "    else:\n",
    "        return torch.eye(X2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimwise False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstrained_lengthscales = nn.Parameter(torch.ones(size=(16,),requires_grad=True))\n",
    "unconstrained_variance = nn.Parameter(torch.ones(size=(1,)), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randint(5, (50,16)) #inducing\n",
    "X2 = torch.randint(5, (25,16)) #data\n",
    "X2=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_dist = square_dist(X, X2)  # (N,M)\n",
    "K2 = torch.exp(-0.5 * sq_dist) # (M,N)\n",
    "K2 = K2.unsqueeze(0) # (1,M,N)\n",
    "diff = difference_matrix_a(X, X2) #(M,N,D_in)\n",
    "diff1 = torch.permute(diff, (0,2,1)) # (M, D_in, N)\n",
    "K1_term = torch.einsum('mnd, mdn -> dmn', diff, diff1) # (D_in,M,N) #TODO not sure if this is correct\n",
    "K3 = (16 - 1.0) - sq_dist # (M,N)\n",
    "K3 = K3 @ identity(X,X2) # M,N\n",
    "K3 = K3.unsqueeze(0) # 1, M, N\n",
    "K = (K1_term + K3) * K2 # D_in, M, N\n",
    "K = torch.permute(K,(1,2,0)) # M,N,D_in\n",
    "l2 = torch.permute((1.0/torch.pow(softplus(unconstrained_lengthscales),2).unsqueeze(0)), (1,0))\n",
    "K = K @ l2\n",
    "K = K @ variance(unconstrained_variance).unsqueeze(-1)\n",
    "# factor =  (variance(unconstrained_variance)/torch.pow(softplus(unconstrained_lengthscales),2).unsqueeze(0)) #1,D_in\n",
    "# factor = torch.permute(factor, (1,0)) # D_in, 1\n",
    "# K = (K @ factor).squeeze() #M,N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimwise True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstrained_lengthscales = nn.Parameter(torch.ones(size=(8,16),requires_grad=True))\n",
    "unconstrained_variance = nn.Parameter(torch.ones(size=(8,)), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randint(5, (50,16)) #inducing\n",
    "X2 = torch.randint(5, (25,16)) #data\n",
    "#X2=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_dist = square_dist_dimwise(X, X2) # (D_out, M,N)\n",
    "K2 = torch.exp(-0.5 * sq_dist) # (D_out, M,N)\n",
    "K2 = K2.unsqueeze(0) # (1, D_out,M,N)\n",
    "diff = difference_matrix_dimwise(X, X2) #(D_out, M,N,D_in)\n",
    "diff1 = torch.permute(diff, (0,1,3,2)) # (D_out,M, D_in, N)\n",
    "K1_term = torch.einsum('dmni, dmin -> idmn', diff, diff1) # (D_in, D_out,M,N) #TODO not sure if this is correct\n",
    "K3 = (16 - 1.0) - sq_dist # (D_out,M,N)\n",
    "K3 = K3 @ identity(X,X2) # D_out,M,N\n",
    "K3 = K3.unsqueeze(0) # 1,D_out,M, N\n",
    "K = (K1_term + K3) * K2 # D_in,D_out, M, N\n",
    "K = torch.permute(K,(1,2,3,0)) # D_out,M,N,D_in\n",
    "l2 = torch.permute((1.0/torch.pow(softplus(unconstrained_lengthscales),2)), (1,0))\n",
    "K = K @ l2\n",
    "K = K @ variance(unconstrained_variance).unsqueeze(-1)\n",
    "K =  K.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
